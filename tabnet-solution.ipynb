{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4e21270",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-13T03:55:55.983672Z",
     "iopub.status.busy": "2024-11-13T03:55:55.982987Z",
     "iopub.status.idle": "2024-11-13T03:56:09.000502Z",
     "shell.execute_reply": "2024-11-13T03:56:08.999422Z"
    },
    "papermill": {
     "duration": 13.023915,
     "end_time": "2024-11-13T03:56:09.002843",
     "exception": false,
     "start_time": "2024-11-13T03:55:55.978928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-tabnet\r\n",
      "  Downloading pytorch_tabnet-4.1.0-py3-none-any.whl.metadata (15 kB)\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from pytorch-tabnet) (1.26.4)\r\n",
      "Requirement already satisfied: scikit_learn>0.21 in /opt/conda/lib/python3.10/site-packages (from pytorch-tabnet) (1.2.2)\r\n",
      "Requirement already satisfied: scipy>1.4 in /opt/conda/lib/python3.10/site-packages (from pytorch-tabnet) (1.14.1)\r\n",
      "Requirement already satisfied: tqdm>=4.36 in /opt/conda/lib/python3.10/site-packages (from pytorch-tabnet) (4.66.4)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit_learn>0.21->pytorch-tabnet) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit_learn>0.21->pytorch-tabnet) (3.5.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\r\n",
      "Downloading pytorch_tabnet-4.1.0-py3-none-any.whl (44 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: pytorch-tabnet\r\n",
      "Successfully installed pytorch-tabnet-4.1.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-tabnet torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ea0d5df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T03:56:09.011110Z",
     "iopub.status.busy": "2024-11-13T03:56:09.010798Z",
     "iopub.status.idle": "2024-11-13T04:50:27.792513Z",
     "shell.execute_reply": "2024-11-13T04:50:27.791534Z"
    },
    "papermill": {
     "duration": 3258.788572,
     "end_time": "2024-11-13T04:50:27.794977",
     "exception": false,
     "start_time": "2024-11-13T03:56:09.006405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling missing values...\n",
      "Encoding activities...\n",
      "Scaling features...\n",
      "Data shapes: X_train: (177024, 507), y_train: (177024, 1)\n",
      "epoch 0  | loss: 10.69026| val_0_mse: 8.75371 |  0:00:11s\n",
      "epoch 10 | loss: 4.27385 | val_0_mse: 4.31102 |  0:01:55s\n",
      "epoch 20 | loss: 3.60645 | val_0_mse: 3.91229 |  0:03:42s\n",
      "epoch 30 | loss: 2.78944 | val_0_mse: 3.61913 |  0:05:26s\n",
      "epoch 40 | loss: 2.02134 | val_0_mse: 3.2076  |  0:07:11s\n",
      "epoch 50 | loss: 1.58616 | val_0_mse: 2.95995 |  0:08:57s\n",
      "epoch 60 | loss: 1.30268 | val_0_mse: 2.69373 |  0:10:44s\n",
      "epoch 70 | loss: 1.11255 | val_0_mse: 2.59059 |  0:12:29s\n",
      "epoch 80 | loss: 0.98643 | val_0_mse: 2.42659 |  0:14:14s\n",
      "epoch 90 | loss: 0.88335 | val_0_mse: 2.29879 |  0:16:01s\n",
      "epoch 100| loss: 0.83308 | val_0_mse: 2.2167  |  0:17:47s\n",
      "epoch 110| loss: 0.77337 | val_0_mse: 2.15345 |  0:19:32s\n",
      "epoch 120| loss: 0.72371 | val_0_mse: 2.07075 |  0:21:16s\n",
      "epoch 130| loss: 0.6903  | val_0_mse: 2.02353 |  0:22:59s\n",
      "epoch 140| loss: 0.65278 | val_0_mse: 1.95731 |  0:24:43s\n",
      "epoch 150| loss: 0.60877 | val_0_mse: 1.91901 |  0:26:29s\n",
      "epoch 160| loss: 0.60454 | val_0_mse: 1.87124 |  0:28:16s\n",
      "epoch 170| loss: 0.57257 | val_0_mse: 1.82591 |  0:30:03s\n",
      "epoch 180| loss: 0.56109 | val_0_mse: 1.81372 |  0:31:50s\n",
      "epoch 190| loss: 0.55836 | val_0_mse: 1.84501 |  0:33:34s\n",
      "epoch 200| loss: 0.53279 | val_0_mse: 1.72059 |  0:35:19s\n",
      "epoch 210| loss: 0.51776 | val_0_mse: 1.69605 |  0:37:03s\n",
      "epoch 220| loss: 0.41397 | val_0_mse: 1.6213  |  0:38:48s\n",
      "epoch 230| loss: 0.41285 | val_0_mse: 1.59508 |  0:40:32s\n",
      "epoch 240| loss: 0.39163 | val_0_mse: 1.5902  |  0:42:14s\n",
      "epoch 250| loss: 0.38773 | val_0_mse: 1.58032 |  0:43:58s\n",
      "epoch 260| loss: 0.38614 | val_0_mse: 1.5769  |  0:45:42s\n",
      "epoch 270| loss: 0.38211 | val_0_mse: 1.56633 |  0:47:27s\n",
      "epoch 280| loss: 0.39601 | val_0_mse: 1.55353 |  0:49:11s\n",
      "epoch 290| loss: 0.37594 | val_0_mse: 1.54793 |  0:50:57s\n",
      "Stop training because you reached max_epochs = 300 with best_epoch = 299 and best_val_0_mse = 1.5053\n",
      "Making predictions...\n",
      "\n",
      "Validation Metrics:\n",
      "RMSE: 1.226905170345345\n",
      "R2 Score: 0.8335757331953124\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def process_time(df):\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df['hour'] = df['time'].dt.hour\n",
    "    df['minute'] = df['time'].dt.minute\n",
    "    df['second'] = df['time'].dt.second\n",
    "    return df\n",
    "\n",
    "def encode_activities(train_df, test_df, activity_cols):\n",
    "    le = LabelEncoder()\n",
    "    all_activities = ['missing']\n",
    "    for col in activity_cols:\n",
    "        all_activities.extend(train_df[col].dropna().unique())\n",
    "        if test_df is not None:\n",
    "            all_activities.extend(test_df[col].dropna().unique())\n",
    "    \n",
    "    le.fit(list(set(all_activities)))\n",
    "    \n",
    "    for col in activity_cols:\n",
    "        train_df[col] = train_df[col].fillna('missing')\n",
    "        train_df[col] = le.transform(train_df[col])\n",
    "        if test_df is not None:\n",
    "            test_df[col] = test_df[col].fillna('missing')\n",
    "            test_df[col] = le.transform(test_df[col])\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "def prepare_data(train_path, test_path=None):\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path) if test_path else None\n",
    "    train_df = process_time(train_df)\n",
    "    if test_df is not None:\n",
    "        test_df = process_time(test_df)\n",
    "    \n",
    "    # Identify column types\n",
    "    bg_cols = [col for col in train_df.columns if col.startswith('bg-')]\n",
    "    insulin_cols = [col for col in train_df.columns if col.startswith('insulin-')]\n",
    "    carbs_cols = [col for col in train_df.columns if col.startswith('carbs-')]\n",
    "    hr_cols = [col for col in train_df.columns if col.startswith('hr-')]\n",
    "    steps_cols = [col for col in train_df.columns if col.startswith('steps-')]\n",
    "    cals_cols = [col for col in train_df.columns if col.startswith('cals-')]\n",
    "    activity_cols = [col for col in train_df.columns if col.startswith('activity-')]\n",
    "    \n",
    "    print(\"Handling missing values...\")\n",
    "    # Handle missing numerical values\n",
    "    numerical_cols = bg_cols + insulin_cols + carbs_cols + hr_cols + steps_cols + cals_cols\n",
    "    for col in numerical_cols:\n",
    "        train_df[col] = train_df[col].fillna(train_df[col].mean())\n",
    "        if test_df is not None:\n",
    "            test_df[col] = test_df[col].fillna(train_df[col].mean())\n",
    "    \n",
    "    print(\"Encoding activities...\")\n",
    "    # Encode activities\n",
    "    train_df, test_df = encode_activities(train_df, test_df, activity_cols)\n",
    "    \n",
    "    print(\"Scaling features...\")\n",
    "    # Scale numerical features\n",
    "    scaler = StandardScaler()\n",
    "    train_df[numerical_cols] = scaler.fit_transform(train_df[numerical_cols])\n",
    "    if test_df is not None:\n",
    "        test_df[numerical_cols] = scaler.transform(test_df[numerical_cols])\n",
    "    \n",
    "    #feature columns\n",
    "    feature_cols = (bg_cols + insulin_cols + carbs_cols + hr_cols + steps_cols + \n",
    "                   cals_cols + activity_cols + ['hour', 'minute', 'second'])\n",
    "    \n",
    "    X_train = train_df[feature_cols].values\n",
    "    y_train = train_df['bg+1:00'].values.reshape(-1, 1)  \n",
    "    \n",
    "    if test_df is not None:\n",
    "        X_test = test_df[feature_cols].values\n",
    "        return X_train, y_train, X_test, feature_cols\n",
    "    else:\n",
    "        return X_train, y_train, None, feature_cols\n",
    "\n",
    "def train_tabnet(X_train, y_train, patience=20, max_epochs=300):\n",
    "    # Split data for validation\n",
    "    X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Initialize tabnet\n",
    "    model = TabNetRegressor(\n",
    "        n_d=64,\n",
    "        n_a=64,\n",
    "        n_steps=5,\n",
    "        gamma=1.5,\n",
    "        n_independent=2,\n",
    "        n_shared=2,\n",
    "        optimizer_fn=torch.optim.Adam,\n",
    "        optimizer_params=dict(lr=1e-2),\n",
    "        mask_type='entmax',\n",
    "        lambda_sparse=1e-3,\n",
    "        scheduler_params=dict(\n",
    "            mode=\"min\",\n",
    "            patience=patience//2,\n",
    "            min_lr=1e-5,\n",
    "            factor=0.5,\n",
    "        ),\n",
    "        scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "        verbose=10\n",
    "    )\n",
    "    \n",
    "    # Train \n",
    "    model.fit(\n",
    "        X_train_split, y_train_split,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        max_epochs=max_epochs,\n",
    "        patience=patience,\n",
    "        batch_size=1024,\n",
    "        virtual_batch_size=128,\n",
    "        num_workers=0,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def main():\n",
    "  \n",
    "    X_train, y_train, X_test, feature_cols = prepare_data('/kaggle/input/brist1d/train.csv', '/kaggle/input/brist1d/test.csv')\n",
    "    \n",
    "    print(f\"Data shapes: X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "    \n",
    "    model = train_tabnet(X_train, y_train)\n",
    "    \n",
    "    print(\"Making predictions...\")\n",
    "    #predictions\n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "  \n",
    "    test_df = pd.read_csv('/kaggle/input/brist1d/test.csv')\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_df['id'],\n",
    "        'bg+1:00': preds.squeeze()  # Convert back to 1D array for submission\n",
    "    })\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=42\n",
    "    )\n",
    "    val_preds = model.predict(X_val)\n",
    "    \n",
    "    print(\"\\nValidation Metrics:\")\n",
    "    print(\"RMSE:\", np.sqrt(mean_squared_error(y_val, val_preds)))\n",
    "    print(\"R2 Score:\", r2_score(y_val, val_preds))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24133e4",
   "metadata": {
    "papermill": {
     "duration": 0.006211,
     "end_time": "2024-11-13T04:50:27.808326",
     "exception": false,
     "start_time": "2024-11-13T04:50:27.802115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf3ff48",
   "metadata": {
    "papermill": {
     "duration": 0.00553,
     "end_time": "2024-11-13T04:50:27.819488",
     "exception": false,
     "start_time": "2024-11-13T04:50:27.813958",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4145e379",
   "metadata": {
    "papermill": {
     "duration": 0.005675,
     "end_time": "2024-11-13T04:50:27.830754",
     "exception": false,
     "start_time": "2024-11-13T04:50:27.825079",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 9553358,
     "sourceId": 82611,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3276.970231,
   "end_time": "2024-11-13T04:50:30.221323",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-13T03:55:53.251092",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
